{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistilBERT Fine Tuning\n",
    "\n",
    "Tune the DistilBERT model for a token classification problem on the `nlpaueb/finer-139` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "import functools\n",
    "import shutil\n",
    "from collections import Counter\n",
    "\n",
    "import huggingface_hub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from evaluate import load as load_metric\n",
    "from matplotlib import pyplot as plt\n",
    "from onnxruntime import InferenceSession\n",
    "from optimum.onnxruntime import ORTModelForTokenClassification\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "sns.set()\n",
    "sns.set_palette(sns.color_palette(\"colorblind\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_hub.notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define constants to make them configurable on the run. The constants are grouped by the following:\n",
    "\n",
    "*   Input related constants\n",
    "*   Training related constants\n",
    "*   Output related constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS_FILE = \"stopwords.txt\"\n",
    "BASE_MODEL_CHECKPOINT = \"distilbert-base-uncased\"\n",
    "DATA_SOURCE = \"nlpaueb/finer-139\"\n",
    "DATA_CACHE = \".cache\"\n",
    "\n",
    "OUTPUT_MODEL_NAME = f\"{BASE_MODEL_CHECKPOINT}-on-mini-finer\"\n",
    "TRAINED_MODEL_CHECKPOINT = f\"checkpoints/{OUTPUT_MODEL_NAME}\"\n",
    "BATCH_SIZE = 16\n",
    "USE_CPU = False\n",
    "N_EPOCHS = 20\n",
    "\n",
    "HUGGING_FACE_REPOSITORY = f\"baluyotraf/{OUTPUT_MODEL_NAME}\"\n",
    "ONNX_OUTPUT_PATH = f\"onnx/{OUTPUT_MODEL_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch uses -100 to present labels related to padding. Define it a constant for ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTORCH_IGNORE = -100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the `finer-139` dataset using the `datasets` library from Hugging Face and check the labels from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(DATA_SOURCE, cache_dir=DATA_CACHE)\n",
    "labels = data[\"train\"].features[\"ner_tags\"].feature.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the data to a `pandas.DataFrame` to have easier access to the different data utilities it offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data[\"train\"].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function is defined to calculate the word count for each of the tagged token. This should help on getting an intuition on what the label means. \n",
    "\n",
    "Aside from the token and tags, the `calculate_word_count_per_tag` function also has a `surround` parameter to allow adding the surrounding tokens to the count. This should allow viewing the surrounding words to get more context about the label. A `count_filter` parameter is also provided to remove possible words that do not provide a lot of information, like stop words.\n",
    "\n",
    "A `filter_no_information_words` was also defined to help in getting most sense of the labels. The rules were defined iteratively based on the data, but generally it's removal or stopwords, months, and money related symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_word_count_per_tag(df, tokens, tags, surround=(0, 1), count_filter=lambda w: True):\n",
    "    words_on_tag = {}\n",
    "    for tokens_, tags_ in zip(df[tokens], df[tags]):\n",
    "        tokens_ = [token_.lower() for token_ in tokens_]\n",
    "        for idx, tag_ in enumerate(tags_):\n",
    "            if tag_ != 0:\n",
    "                min_idx, max_idx = (s + idx for s in surround)\n",
    "                min_idx = max(0, min_idx)\n",
    "                max_idx = min(len(tokens_), max_idx)\n",
    "\n",
    "                words_on_tag.setdefault(tag_, []).extend(tokens_[min_idx:max_idx])\n",
    "\n",
    "    word_count_on_tag = {\n",
    "        tag_: Counter((token_ for token_ in tokens_ if count_filter(token_))) for tag_, tokens_ in words_on_tag.items()\n",
    "    }\n",
    "    return word_count_on_tag\n",
    "\n",
    "\n",
    "STOPWORDS = set()\n",
    "with open(STOPWORDS_FILE) as f:\n",
    "    for line in f.readlines():\n",
    "        STOPWORDS.add(line.strip())\n",
    "\n",
    "CALENDAR_NAMES = {calendar.month_name[idx].lower() for idx in range(0, 13)}\n",
    "NUMERIC_WORDS = {\n",
    "    \"million\",\n",
    "    \"billion\",\n",
    "}\n",
    "NUMERIC_SYMBOLS = {\",\", \"$\", \".\", \"-\", \"%\"}\n",
    "NO_INFORMATION_WORDS = STOPWORDS | CALENDAR_NAMES | NUMERIC_WORDS | NUMERIC_SYMBOLS\n",
    "\n",
    "\n",
    "def filter_no_information_words(w):\n",
    "    try:\n",
    "        float(w)\n",
    "        return False\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    if w in NO_INFORMATION_WORDS:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "word_count_on_tag = calculate_word_count_per_tag(\n",
    "    df=data_df, tokens=\"tokens\", tags=\"ner_tags\", surround=(-5, 6), count_filter=filter_no_information_words\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of sentences containing the tag was also computed. The unique tags per sentence were extracted and then counted. The count only focused on the `B-` labels as they mark the beginning of an entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_counts_per_row = data_df[\"ner_tags\"].map(lambda r: set(r))\n",
    "ner_counts = Counter(ner_counts_per_row.explode())\n",
    "ner_count_names_df = pd.DataFrame(\n",
    "    [\n",
    "        {\"idx\": idx, \"count\": count, \"label\": labels[idx]}\n",
    "        for idx, count in ner_counts.items()\n",
    "        if labels[idx].startswith(\"B\")\n",
    "    ]\n",
    ").sort_values(\"count\", ascending=True)\n",
    "ner_count_names_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The focus will be the labels with the fewest members. This was mostly done for computation time and resources reasons but the code can be extended to any number of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ner_df = ner_count_names_df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the smallest labels does not have a large difference between them. The samples are kept the same without using any augmentation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_count_distribution(df, x, y):\n",
    "    ax = sns.barplot(df, x=x, y=y)\n",
    "    ax.set_title(\"Count of the target labels\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_count_distribution(target_ner_df, x=\"count\", y=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The common words around the labels are printed out to get more understanding of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, label in zip(target_ner_df[\"idx\"], target_ner_df[\"label\"]):\n",
    "    print(label)\n",
    "    for word, count in word_count_on_tag[idx].most_common(20):\n",
    "        print(f\"\\t{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nlpaueb/finer-139` data contained a lot of labels however, this exercise only used 4 labels. This means that there was a need to remap the labels. To do this, a mapping was created with the index of the label in the data to the 4 label problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label_idxs = set(target_ner_df[\"idx\"])\n",
    "target_label_names = set(target_ner_df[\"label\"].str[2:])\n",
    "\n",
    "target_ner_tag_map = {\n",
    "    old: new for new, old in enumerate((idx for idx, label in enumerate(labels) if label[2:] in target_label_names), 1)\n",
    "}\n",
    "target_labels = [labels[idx] for idx in target_ner_tag_map.keys()]\n",
    "\n",
    "target_id_to_label = dict(enumerate([labels[0], *target_labels]))\n",
    "target_label_to_id = {label: id_ for id_, label in target_id_to_label.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was filtered to remove the data without the target tags. This was not mandatory, but was also done for the sake of computation time and resources. The labels were also mapped to the new labels, thus, a column `target_ner_tags` was created from the `ner_tag` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_ner_tags(row):\n",
    "    new_tags = [target_ner_tag_map.get(tag, 0) for tag in row[\"ner_tags\"]]\n",
    "    return {\"target_ner_tags\": new_tags}\n",
    "\n",
    "\n",
    "target_data = data.filter(lambda x: set(x[\"ner_tags\"]) & target_label_idxs)\n",
    "target_data = target_data.map(remap_ner_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tokenizer was loaded from the `DistilBERT` repository and a function to tokenize the words and align the labels was defined. The alignment was performed since the tokenizer can split up words to subwords and can produce more tokens than the words.\n",
    "\n",
    "The implementation of the `tokenize_and_align_labels` was mostly taken from the `DistilBERT` documentation, with some minor refactors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_CHECKPOINT)\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples, tokenizer, label_all_tokens=True):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"target_ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(PYTORCH_IGNORE)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else PYTORCH_IGNORE)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_target_data = target_data.map(lambda rows: tokenize_and_align_labels(rows, tokenizer), batched=True)\n",
    "tokenized_target_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A token classification was defined using the `DistilBERT`. The number of labels and the mapping were defined to fit the smaller problem defined here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    BASE_MODEL_CHECKPOINT, num_labels=len(target_id_to_label), id2label=target_id_to_label, label2id=target_label_to_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training arguments were defined. Evaluation, checkpoints and logging were done at each epoch. The best model was also saved in the end. Otherwise, the values were taken from the `DistilBERT` documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    TRAINED_MODEL_CHECKPOINT,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=N_EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    use_cpu=USE_CPU,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric helpers for model evaluation were defined. There are three types of metrics defined:\n",
    "\n",
    "*   Sequence metrics with per label metrics\n",
    "*   Overall metrics for training\n",
    "*   Confusion matrix for complete prediction picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "\n",
    "def compute_metrics(predictions, labels, id2label):\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [id2label[pred] for (pred, lbl) in zip(prediction, label) if lbl != PYTORCH_IGNORE]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[lbl] for (pred, lbl) in zip(prediction, label) if lbl != PYTORCH_IGNORE]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    return metric.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "\n",
    "def compute_training_metrics(p, id2label):\n",
    "    results = compute_metrics(*p, id2label)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(predictions, labels, names=None, normalize=None):\n",
    "    flat_predictions = np.asarray(predictions).argmax(-1).reshape(-1)\n",
    "    flat_labels = np.asarray(labels).reshape(-1)\n",
    "\n",
    "    valid_labels = flat_labels != PYTORCH_IGNORE\n",
    "\n",
    "    confusion_matrix = sklearn.metrics.confusion_matrix(\n",
    "        flat_labels[valid_labels], flat_predictions[valid_labels], normalize=normalize\n",
    "    )\n",
    "    display = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix, display_labels=names)\n",
    "    display.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `DataCollatorForTokenClassification` was defined to save memory. This padded based on the batch length rather than the global maximum length.\n",
    "\n",
    "This was used on the `Trainer` that used the training args, the tokenized dataset, and the metrics function for training. The tokenizer was also provided for proper padding and future fine tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(TRAINED_MODEL_CHECKPOINT, ignore_errors=True)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_target_data[\"train\"],\n",
    "    eval_dataset=tokenized_target_data[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=functools.partial(compute_training_metrics, id2label=target_id_to_label),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the performance of the model before training. This was a very bad performance since the out-of-the-box model was not trained on any labels before hand. This is only a benchmark to see the performance improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = trainer.predict(tokenized_target_data[\"test\"])\n",
    "compute_metrics(test_output.predictions, test_output.label_ids, target_id_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training. Training metrics were displayed along side the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_result = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the performance of the model to verify the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = trainer.predict(tokenized_target_data[\"test\"])\n",
    "compute_metrics(test_output.predictions, test_output.label_ids, target_id_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the data to the `Hugging Face` repository. This made the model publicly available and easily reusable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(HUGGING_FACE_REPOSITORY)\n",
    "tokenizer.push_to_hub(HUGGING_FACE_REPOSITORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an `ONNX Runtime` model using the `Hugging Face` utilities. The tokenizer was also saved on the same path so that the complete pipeline can be read from the same path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_model = ORTModelForTokenClassification.from_pretrained(HUGGING_FACE_REPOSITORY, export=True)\n",
    "\n",
    "shutil.rmtree(ONNX_OUTPUT_PATH, ignore_errors=True)\n",
    "ort_model.save_pretrained(ONNX_OUTPUT_PATH)\n",
    "tokenizer.save_pretrained(ONNX_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A utitilty function `predict_from_model` predicts the labels with a model, tokenizer, and the tokenized data. This helped in comparing the `ONNX` model with the PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_model(model, tokenizer, tokenized_data):\n",
    "    predictions = model(\n",
    "        input_ids=torch.tensor(tokenized_data[\"input_ids\"], device=model.device),\n",
    "        attention_mask=torch.tensor(tokenized_data[\"attention_mask\"], device=model.device),\n",
    "    )\n",
    "    return predictions.logits.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A padded data was created for prediction purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_padded_test_data = target_data[\"test\"].map(\n",
    "    lambda rows: tokenize_and_align_labels(rows, functools.partial(tokenizer, padding=\"longest\")), batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics calculated from the `ONNX Runtime` model seems to match the training results. To get more information about the predictions, the confusion matrix was also extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_pred = predict_from_model(ort_model, tokenizer, tokenized_padded_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(ort_pred, tokenized_padded_test_data[\"labels\"], target_id_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(ort_pred, tokenized_padded_test_data[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual PyTorch model was also tested instead of relying on the `Trainer.predict` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    cpu_pred = predict_from_model(cpu_model, tokenizer, tokenized_padded_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ONNX Runtime` model was also run using the `ONNX Runtime` API to make sure that the result was stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = InferenceSession(f\"{ONNX_OUTPUT_PATH}/model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_output = ort_session.run(\n",
    "    output_names=[\"logits\"],\n",
    "    input_feed={key: tokenized_padded_test_data[key] for key in [\"input_ids\", \"attention_mask\"]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(ort_output[0], tokenized_padded_test_data[\"labels\"], target_id_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(ort_output[0], tokenized_padded_test_data[\"labels\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
